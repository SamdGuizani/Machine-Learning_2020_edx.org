{"cells":[{"metadata":{"_uuid":"989ea979-09c6-4ca3-8241-e5c5aae88c0d","_cell_guid":"c59ed07b-6fa3-43a4-af1b-fe738198cec1","trusted":true},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# %% [code]\n## Import test data\nX_test = np.genfromtxt(\"../input/X_test_classification.csv\", delimiter=\",\") # 30 obs, 5 variables\nX_train = np.genfromtxt(\"../input/X_train_classification.csv\", delimiter=\",\") # 50 obs, 5 variables\ny_train = np.genfromtxt(\"../input/y_train_classification.csv\", delimiter=\",\") # 50 obs. 1 variable (2 classes: 0, 1)\n\n# %% [code]\nfrom __future__ import division\nimport numpy as np\nimport sys\n\n'''X_train = np.genfromtxt(sys.argv[1], delimiter=\",\")\ny_train = np.genfromtxt(sys.argv[2])\nX_test = np.genfromtxt(sys.argv[3], delimiter=\",\")'''\n\n## can make more functions if required\ndef empirical_mean(X, y):\n    # Function returning the MLE of mean for each class\n    classes = np.unique(y)\n    mu_hat = []\n    for c in classes:\n        X_extract = X[np.where(y == c)[0]]\n        mu_hat.append(np.mean(X_extract, axis=0))\n    return mu_hat\n\ndef empirical_covariance(X, y):\n    # Function returning the MLE of covariance matrix for each class\n    classes = np.unique(y)\n    sigma_hat = []\n    for c in classes:\n        X_extract = X[np.where(y == c)[0]]\n        n = np.shape(X_extract)[0]\n        mu_class = np.mean(X_extract, axis=0)\n        sigma_hat.append((1 / n) * np.dot(np.transpose(X_extract - mu_class), (X_extract - mu_class)))\n    return sigma_hat\n\ndef empirical_prior(y):\n    # Function returning the prior pi of each class\n    n = np.shape(y)[0]\n    classes = np.unique(y)\n    pi_hat = []\n    for c in classes:\n        pi_hat.append(np.shape(np.where(y == c))[1] / n)\n    return pi_hat\n\ndef empirical_likelihood(row, mu_y, sigma_y):\n    # Function returning the likelihood of a specific row given y class distribution i.e. p(xi|yi)\n    likelihood = np.linalg.det(sigma_y)**(-1/2) * np.exp((-1/2) * np.dot(np.dot((row - mu_y), np.linalg.inv(sigma_y)), np.transpose(row - mu_y)))\n    return likelihood\n\ndef empirical_marginal(row, mu_hat, sigma_hat, pi_hat):\n    # Function returning the marginal of a specific row i.e. p(xi) = sum over the classes(p(xi|yi))\n    marginal = 0\n    for j in range(len(mu_hat)):\n        marginal = marginal + pi_hat[j] * empirical_likelihood(row, mu_hat[j], sigma_hat[j])\n    return marginal\n\ndef pluginClassifier(X_train, y_train, X_test):    \n    # this function returns the required output\n    mu_hat = empirical_mean(X_train, y_train)\n    sigma_hat = empirical_covariance(X_train, y_train)\n    pi_hat = empirical_prior(y_train)\n    final_outputs = []\n    for row in X_test:\n        marginal = empirical_marginal(row, mu_hat, sigma_hat, pi_hat)\n        p = []\n        for j in range(len(mu_hat)):\n            likelihood = empirical_likelihood(row, mu_hat[j], sigma_hat[j])\n            # Posterior probability that row (from X_test) belongs to calss j\n            p.append(pi_hat[j] * likelihood / marginal)\n        final_outputs.append(p)\n    final_outputs = np.array(final_outputs)\n    return final_outputs\n \n\nfinal_outputs = pluginClassifier(X_train, y_train, X_test) # assuming final_outputs is returned from function\n\nnp.savetxt(\"probs_test.csv\", final_outputs, delimiter=\",\") # write output to file\n\n# %% [code] {\"_kg_hide-output\":true}\nfor c in np.unique(y_train):\n    print('empirical prior class ', int(c),' = \\n', empirical_prior(y_train)[int(c)], '\\n Type =', type(empirical_prior(y_train)[int(c)]))\n    print('empirical mean class ', int(c),' = \\n', empirical_mean(X_train, y_train)[int(c)], '\\n Type =', type(empirical_mean(X_train, y_train)[int(c)]))\n    print('empirical covariance class ', int(c),' = \\n', empirical_covariance(X_train, y_train)[int(c)], '\\n Type =', type(empirical_covariance(X_train, y_train)[int(c)]))\n\n# %% [code]\nprint('final outputs = \\n', final_outputs)","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}